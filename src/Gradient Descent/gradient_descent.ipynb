{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_dataset\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 ###\n",
    "Load, analyse and preprocess the CIFAR-10 dataset. Split it into 3 datasets: training, validation and test. Take a subset of these datasets by keeping only 2 labels: bird and plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset: <class 'torchvision.datasets.cifar.CIFAR10'>\n",
      "Size of the train dataset:       , 45000\n",
      "Size of the validation dataset:  , 5000\n",
      "Size of the test dataset:        , 10000\n"
     ]
    }
   ],
   "source": [
    "cifar2_train, cifar2_val, cifar2_test = load_dataset(CIFAR10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 ###\n",
    "Write a MyMLP class that implements a MLP in PyTorch (so only fully connected layers) such\n",
    "that:\n",
    "\n",
    "    (a) The input dimension is 3072 (= 32*32*3) and the output dimension is 2 (for the 2\n",
    "    classes).\n",
    "\n",
    "    (b) The hidden layers have respectively 512, 128 and 32 hidden units.\n",
    "\n",
    "    (c) All activation functions are ReLU. The last layer has no activation function since the\n",
    "    cross-entropy loss already includes a softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, 512)\n",
    "        self.fc2 = nn.Linear(512,128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.flatten(x,1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = torch.relu(self.fc2(out))\n",
    "        out = torch.relu(self.fc3(out))\n",
    "        out = self.fc4(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 ###\n",
    "Write a train(n epochs, optimizer, model, loss fn, train loader) function that trains\n",
    "model for n epochs epochs given an optimizer optimizer, a loss function loss fn and a dataloader train loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "\n",
    "    device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "        else torch.device('cpu'))\n",
    "    \n",
    "    print(f\"Training on device {device}.\")\n",
    "\n",
    "    n_batch = len(train_loader)\n",
    "    \n",
    "    # We'll store there the training loss for each epoch\n",
    "    losses_train = []\n",
    "    \n",
    "    # Set the network in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Re-initialize gradients, just in case the model has been inappropriately \n",
    "    # manipulated before the training\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1): \n",
    "        \n",
    "        # Training loss for the current epoch\n",
    "        loss_train = 0\n",
    "\n",
    "        # Loop over our dataset (in batches the data loader creates for us)\n",
    "        for imgs, labels in train_loader:\n",
    "            \n",
    "            imgs = imgs.to(device=device, dtype=torch.double) \n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            # Feed a batch into our model\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            # Compute the loss we wish to minimize \n",
    "            # Note that by default, it is the mean loss that is computed\n",
    "            # (so entire_batch_loss / batch_size)\n",
    "            loss = loss_fn(outputs, labels) \n",
    "            \n",
    "            \n",
    "            # Perform the backward step. That is, compute the gradients of all parameters we want the network to learn\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the model\n",
    "            optimizer.step() \n",
    "            \n",
    "            # Zero out gradients before the next round (or the end of training)\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            # Update loss for this epoch\n",
    "            # It is important to transform the loss to a number with .item()\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        # Store current epoch loss. \n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return losses_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_manual_update(n_epochs, lr, model, loss_fn, train_loader, weight_decay=0, momentum=0):\n",
    "    \n",
    "    # Train on GPU if available\n",
    "    device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "    \n",
    "    n_batch = len(train_loader)\n",
    "    \n",
    "    # Need to store the training loss\n",
    "    losses_train = []\n",
    "    \n",
    "    # Will be used when applying momentum\n",
    "    velocities = {}\n",
    "    \n",
    "    # Set the network in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Re-initialize gradient just in case they have been inappropriately \n",
    "    # manipulated before the training\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        # Training loss for current epoch\n",
    "        loss_train = 0\n",
    "        \n",
    "        # Loop over our dataset (in batches the data loader creates for us)\n",
    "        for imgs, labels in train_loader:\n",
    "            \n",
    "            imgs.to(device=device, dtype=torch.double)\n",
    "            labels.to(device=device)\n",
    "            \n",
    "            # Feed a batch into the model\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            # Compute the loss we want to minimize\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Perform the backward step\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the model\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Want to update the weights and biases\n",
    "                for name, p in model.named_parameters():\n",
    "                    grad = p.grad\n",
    "                    \n",
    "                    # L2 regularization\n",
    "                    if weight_decay:\n",
    "                        grad += weight_decay * p.data\n",
    "                    \n",
    "                    # Momentum\n",
    "                    if momentum:\n",
    "                        if name not in velocities:\n",
    "                            # Want to store a tensor separate from the graph\n",
    "                            buf = velocities[name] = torch.clone(grad).detach()\n",
    "                        else:\n",
    "                            buf = velocities[name]\n",
    "                            buf.mul_(momentum).add_(grad)\n",
    "                        grad = buf\n",
    "                    \n",
    "                    # Learning step\n",
    "                    p.data -= grad * lr\n",
    "                    \n",
    "                # Zero out the gradients\n",
    "                model.zero_grad()\n",
    "            # Update the loss for this epoch\n",
    "            # Need to use .item() to transform the loss into a number\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        # Store current epoch loss\n",
    "        losses_train.append(loss_train / n_batch)\n",
    "        \n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "            \n",
    "    return losses_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 ###\n",
    "Train 2 instances of MyMLP, one using train and the other using train manual update (use\n",
    "the same parameter values for both models). Compare their respective training losses. To get\n",
    "exactly the same results with both functions, see section 3.3.\n",
    "\n",
    "Note: only done for train,since have not made train_manual_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 128\n",
    "lr = 1e-1\n",
    "weight_decay = 1e-3\n",
    "momentum = 0.9\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_train = MyMLP().to(device=device) \n",
    "torch.manual_seed(123)\n",
    "model_manual_train = MyMLP().to(device=device) \n",
    "\n",
    "optimizer = optim.SGD(model_train.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n",
      "14:16:38.135411  |  Epoch 1  |  Training loss 0.499\n",
      "14:16:42.601544  |  Epoch 5  |  Training loss 0.337\n",
      "14:16:48.483844  |  Epoch 10  |  Training loss 0.277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4991299856039895,\n",
       " 0.4179808006687227,\n",
       " 0.38310016322177143,\n",
       " 0.35417560659043956,\n",
       " 0.3369651512339421,\n",
       " 0.31428151765006895,\n",
       " 0.30157356896140336,\n",
       " 0.3016805296169667,\n",
       " 0.27999407095439194,\n",
       " 0.27655556492489153]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(n_epochs, optimizer, model_train, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:16:49.921635  |  Epoch 1  |  Training loss 0.499\n",
      "14:16:55.285176  |  Epoch 5  |  Training loss 0.337\n",
      "14:17:02.098553  |  Epoch 10  |  Training loss 0.277\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4991299856039894,\n",
       " 0.4179808006687227,\n",
       " 0.38310016322177143,\n",
       " 0.35417560659043956,\n",
       " 0.33696515123394216,\n",
       " 0.31428151765006895,\n",
       " 0.30157356896140325,\n",
       " 0.30168052961696673,\n",
       " 0.27999407095439205,\n",
       " 0.27655556492489247]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_manual_update(n_epochs, lr, model_manual_train, loss_fn, train_loader, weight_decay, momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.10 ###\n",
    "Evaluate the best model and analyse its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "def compute_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # We do not want gradients here, as we will not want to update the parameters.\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "\n",
    "    acc =  correct / total\n",
    "    print(\"Accuracy: {:.2f}\".format(acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to use these global parameters for all 4 models\n",
    "\n",
    "batch_size = 256\n",
    "n_epoch = 100\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "seed = 256\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=batch_size, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:17:03.008989  |  Epoch 1  |  Training loss 0.682\n",
      "14:17:05.892872  |  Epoch 5  |  Training loss 0.563\n",
      "14:17:09.355357  |  Epoch 10  |  Training loss 0.474\n",
      "14:17:12.874920  |  Epoch 15  |  Training loss 0.429\n",
      "14:17:16.418281  |  Epoch 20  |  Training loss 0.390\n",
      "14:17:19.878996  |  Epoch 25  |  Training loss 0.356\n",
      "14:17:23.364676  |  Epoch 30  |  Training loss 0.322\n",
      "14:17:26.809942  |  Epoch 35  |  Training loss 0.289\n",
      "14:17:30.662488  |  Epoch 40  |  Training loss 0.265\n",
      "14:17:34.552870  |  Epoch 45  |  Training loss 0.255\n",
      "14:17:38.373960  |  Epoch 50  |  Training loss 0.235\n",
      "14:17:42.170278  |  Epoch 55  |  Training loss 0.249\n",
      "14:17:46.001764  |  Epoch 60  |  Training loss 0.211\n",
      "14:17:49.717840  |  Epoch 65  |  Training loss 0.161\n",
      "14:17:53.452008  |  Epoch 70  |  Training loss 0.132\n",
      "14:17:57.793272  |  Epoch 75  |  Training loss 0.143\n",
      "14:18:01.713645  |  Epoch 80  |  Training loss 0.136\n",
      "14:18:05.582019  |  Epoch 85  |  Training loss 0.078\n",
      "14:18:09.478264  |  Epoch 90  |  Training loss 0.073\n",
      "14:18:13.236618  |  Epoch 95  |  Training loss 0.184\n",
      "14:18:17.020731  |  Epoch 100  |  Training loss 0.046\n",
      "Accuracy: 0.98\n",
      "Accuracy: 0.85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8494404883011191"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 1\n",
    "lr_1 = 0.01\n",
    "mom_1 = 0\n",
    "decay_1 = 0\n",
    "torch.manual_seed(seed)\n",
    "model_1 = MyMLP().to(device=device) \n",
    "\n",
    "train_manual_update(n_epoch, lr_1, model_1, loss_fn, train_loader, weight_decay=decay_1, momentum=mom_1)\n",
    "compute_accuracy(model_1, train_loader)\n",
    "compute_accuracy(model_1, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
