{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_dataset\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 ###\n",
    "Load, analyse and preprocess the CIFAR-10 dataset. Split it into 3 datasets: training, validation and test. Take a subset of these datasets by keeping only 2 labels: bird and plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_train, cifar10_val, cifar10_test = load_dataset.load_CIFAR10(\"CIFAR10\")\n",
    "cifar2_train, cifar2_val, cifar2_test = load_dataset.subset_dataset(cifar10_train, cifar10_val, cifar10_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size of the training dataset: ', len(cifar2_train))\n",
    "print('Size of the validation dataset: ', len(cifar2_val))\n",
    "print('Size of the test dataset: ', len(cifar2_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = torch.stack([transforms.functional.to_tensor(img) for img, _ in cifar10_train])\n",
    "print(imgs.shape)\n",
    "mean = imgs.mean()\n",
    "std = imgs.std()\n",
    "normalizer = transforms.Normalize(mean, std)\n",
    "print(\"Mean: \", mean, \"std: \", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_preprocessor = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalizer,\n",
    "])\n",
    "cifar10_train, cifar10_val, cifar10_test = load_dataset.load_CIFAR10(\"CIFAR10\", pre_processing=cifar_preprocessor)\n",
    "cifar2_train, cifar2_val, cifar2_test = load_dataset.subset_dataset(cifar10_train, cifar10_val, cifar10_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,3))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Find an image in the dataset with the right label\n",
    "    img = next(img for img, label in cifar2_train if label == i)\n",
    "    # Plot image\n",
    "    ax.imshow(img.permute(1,2,0), cmap='gray')\n",
    "    # Add title\n",
    "    ax.set_title(i)\n",
    "    # Remove ticks\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 ###\n",
    "Write a MyMLP class that implements a MLP in PyTorch (so only fully connected layers) such\n",
    "that:\n",
    "\n",
    "    (a) The input dimension is 3072 (= 32*32*3) and the output dimension is 2 (for the 2\n",
    "    classes).\n",
    "\n",
    "    (b) The hidden layers have respectively 512, 128 and 32 hidden units.\n",
    "\n",
    "    (c) All activation functions are ReLU. The last layer has no activation function since the\n",
    "    cross-entropy loss already includes a softmax activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(32*32*3, 512)\n",
    "        self.fc2 = nn.Linear(512,128)\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.fc4 = nn.Linear(32, 2)\n",
    "    def forward(self, x):\n",
    "        out = torch.flatten(x,1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = torch.relu(self.fc2(out))\n",
    "        out = torch.relu(self.fc3(out))\n",
    "        out = self.fc4(out)\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 ###\n",
    "Write a train(n epochs, optimizer, model, loss fn, train loader) function that trains\n",
    "model for n epochs epochs given an optimizer optimizer, a loss function loss fn and a dataloader train loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader, device=None):\n",
    "    if device is None:\n",
    "        device = (\n",
    "            torch.device('cuda') if torch.cuda.is_available()\n",
    "            else torch.device('cpu'))\n",
    "    print(f\"Training on device {device}.\")\n",
    "\n",
    "    n_batch = len(train_loader)\n",
    "    \n",
    "    # We'll store there the training loss for each epoch\n",
    "    losses_train = []\n",
    "    \n",
    "    # Set the network in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Re-initialize gradients, just in case the model has been inappropriately \n",
    "    # manipulated before the training\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1): \n",
    "        \n",
    "        # Training loss for the current epoch\n",
    "        loss_train = 0\n",
    "\n",
    "        # Loop over our dataset (in batches the data loader creates for us)\n",
    "        for imgs, labels in train_loader:\n",
    "            \n",
    "            imgs = imgs.to(device=device) \n",
    "            labels = labels.to(device=device)\n",
    "            # Feed a batch into our model\n",
    "            outputs = model(imgs)\n",
    "            \n",
    "            # Compute the loss we wish to minimize \n",
    "            # Note that by default, it is the mean loss that is computed\n",
    "            # (so entire_batch_loss / batch_size)\n",
    "            loss = loss_fn(outputs, labels) \n",
    "            \n",
    "            \n",
    "            # Perform the backward step. That is, compute the gradients of all parameters we want the network to learn\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the model\n",
    "            optimizer.step() \n",
    "            \n",
    "            # Zero out gradients before the next round (or the end of training)\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            # Update loss for this epoch\n",
    "            # It is important to transform the loss to a number with .item()\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        # Store current epoch loss. \n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.3f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return losses_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 ###\n",
    "Train 2 instances of MyMLP, one using train and the other using train manual update (use\n",
    "the same parameter values for both models). Compare their respective training losses. To get\n",
    "exactly the same results with both functions, see section 3.3.\n",
    "\n",
    "Note: only done for train,since have not made train_manual_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    torch.device('cuda') if torch.cuda.is_available()\n",
    "    else torch.device('cpu'))\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=batch_size, shuffle=True)\n",
    "print(train_loader)\n",
    "models = []\n",
    "model_names = []\n",
    "losses_train = []\n",
    "\n",
    "model = MyMLP().to(device=device) \n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "loss_train = train(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader\n",
    ")\n",
    "\n",
    "models.append(model)\n",
    "model_names.append('MyMLP, lr=0.1')\n",
    "losses_train.append(loss_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.10 ###\n",
    "Evaluate the best model and analyse its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2_train, batch_size=64, shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "def compute_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # We do not want gradients here, as we will not want to update the parameters.\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "\n",
    "    acc =  correct / total\n",
    "    print(\"Accuracy: {:.2f}\".format(acc))\n",
    "    return acc\n",
    "\n",
    "print(\"Training accuracy:\")\n",
    "compute_accuracy(model, train_loader)\n",
    "print(\"Validation accuracy:\")\n",
    "compute_accuracy(model, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2506301c42d9f1239480ac02b4959d757f5df3f10b238d53272c7995978d710"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
